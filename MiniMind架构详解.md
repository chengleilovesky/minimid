# MiniMind æ¨¡å‹æ¶æ„è¯¦è§£

## 1. æ¨¡å‹æ•´ä½“æ¶æ„

```
                    MiniMind è¯­è¨€æ¨¡å‹æ¶æ„
                           
    è¾“å…¥: input_ids [batch_size, seq_len]
                           â”‚
                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              è¯åµŒå…¥å±‚ (Token Embedding)           â”‚
    â”‚           nn.Embedding(vocab_size, dim)         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
                    Dropout(dropout)
                           â”‚
                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                ä½ç½®ç¼–ç  (RoPE)                   â”‚
    â”‚        precompute_pos_cis(dim//n_heads)        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘              MiniMindBlock Ã— N å±‚                â•‘
    â•‘                                                 â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘  â”‚              æ®‹å·®è¿æ¥ 1                  â”‚    â•‘
    â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â•‘
    â•‘  â”‚  â”‚        RMSNorm (attention)      â”‚    â”‚    â•‘
    â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â•‘
    â•‘  â”‚              â”‚                          â”‚    â•‘
    â•‘  â”‚              â–¼                          â”‚    â•‘
    â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â•‘
    â•‘  â”‚  â”‚        å¤šå¤´æ³¨æ„åŠ› (Attention)    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚  Query  Key   Value     â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚   wq     wk     wv      â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â”‚                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â–¼                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)        â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â”‚                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â–¼                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚   Flash Attentionæˆ–     â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚   æ‰‹åŠ¨æ³¨æ„åŠ›è®¡ç®—         â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â”‚                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â–¼                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚         è¾“å‡ºæŠ•å½± (wo)            â”‚    â”‚    â•‘
    â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•‘                 â”‚                                 â•‘
    â•‘                 â–¼                                 â•‘
    â•‘  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â•‘
    â•‘  â”‚              æ®‹å·®è¿æ¥ 2                  â”‚    â•‘
    â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â•‘
    â•‘  â”‚  â”‚         RMSNorm (ffn)           â”‚    â”‚    â•‘
    â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â•‘
    â•‘  â”‚              â”‚                          â”‚    â•‘
    â•‘  â”‚              â–¼                          â”‚    â•‘
    â•‘  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â•‘
    â•‘  â”‚  â”‚        å‰é¦ˆç½‘ç»œå±‚                â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚                                 â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  æ ‡å‡†FFN:                       â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚    SwiGLU æ¿€æ´»å‡½æ•°       â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚  w2(SiLU(w1(x)) * w3(x)) â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚                                 â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  æˆ– MoE FFN:                   â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚      é—¨æ§ç½‘ç»œ            â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚   (é€‰æ‹© top-k ä¸“å®¶)      â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â”‚                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚              â–¼                  â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚   ä¸“å®¶ç½‘ç»œ Ã— N_experts   â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â”‚   (å¹¶è¡Œå¤„ç†)             â”‚    â”‚    â”‚    â•‘
    â•‘  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â”‚    â•‘
    â•‘  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    â•‘
    â•‘  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
                           â”‚
                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            æœ€ç»ˆå±‚å½’ä¸€åŒ– (RMSNorm)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         è¾“å‡ºæŠ•å½±å±‚ (Linear + æƒé‡å…±äº«)           â”‚
    â”‚        nn.Linear(dim, vocab_size, bias=False)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â–¼
    è¾“å‡º: logits [batch_size, seq_len, vocab_size]
```

## 2. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 2.1 RMSNorm (å‡æ–¹æ ¹å½’ä¸€åŒ–)
```python
class RMSNorm(torch.nn.Module):
    def forward(self, x):
        return self.weight * (x.float() * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)).type_as(x)
```
- **åŸç†**: ç§»é™¤äº† LayerNorm çš„å‡å€¼ä¸­å¿ƒåŒ–ï¼Œåªä¿ç•™æ–¹å·®å½’ä¸€åŒ–
- **å…¬å¼**: `x / sqrt(mean(xÂ²) + eps) * weight`

### 2.2 æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
```python
def precompute_pos_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)
    freqs = torch.outer(t, freqs).float()
    pos_cis = torch.polar(torch.ones_like(freqs), freqs)  # cos + i*sin
    return pos_cis
```
- **åŸç†**: é€šè¿‡å¤æ•°ä¹˜æ³•å°†ä½ç½®ä¿¡æ¯ç›´æ¥ç¼–ç åˆ° query å’Œ key ä¸­
- **ä¼˜åŠ¿**: æ”¯æŒå¤–æ¨åˆ°æ›´é•¿åºåˆ—ï¼Œä½ç½®ä¿¡æ¯æ›´è‡ªç„¶

### 2.3 å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
- **å¤šæŸ¥è¯¢æ³¨æ„åŠ› (MQA)**: `n_kv_heads < n_heads`ï¼Œå‡å°‘ KV ç¼“å­˜å†…å­˜
- **åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA)**: MQA å’Œ MHA çš„æŠ˜ä¸­æ–¹æ¡ˆ
- **Flash Attention**: å†…å­˜é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°

### 2.4 å‰é¦ˆç½‘ç»œå±‚
#### æ ‡å‡† FFN (SwiGLU)
```python
def forward(self, x):
    return self.w2(F.silu(self.w1(x)) * self.w3(x))
```

#### æ··åˆä¸“å®¶ (MoE)
```python
# é—¨æ§æœºåˆ¶é€‰æ‹© top-k ä¸“å®¶
routing_weights, routing_indices = torch.topk(routing_weights, num_experts_per_tok, dim=-1)
```

## 3. è®­ç»ƒæ—¶æ•°æ®æµè½¬å›¾

```
                        MiniMind è®­ç»ƒæ•°æ®æµ
                              
    è®­ç»ƒæ•°æ® [batch_size, seq_len]
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              æ•°æ®é¢„å¤„ç†                          â”‚
    â”‚  â€¢ Tokenization (åˆ†è¯)                         â”‚
    â”‚  â€¢ Padding/Truncation (å¡«å……/æˆªæ–­)               â”‚
    â”‚  â€¢ DataLoader æ‰¹å¤„ç†                           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              å‰å‘ä¼ æ’­                            â”‚
    â”‚                                                 â”‚
    â”‚  input_ids â†’ embedding â†’ MiniMindBlocks â†’ logitsâ”‚
    â”‚                                                 â”‚
    â”‚  ä¸­é—´è®¡ç®—:                                       â”‚
    â”‚  â€¢ æ³¨æ„åŠ›æƒé‡çŸ©é˜µ                               â”‚
    â”‚  â€¢ KV ç¼“å­˜ (å¦‚æœä½¿ç”¨)                          â”‚
    â”‚  â€¢ MoE ä¸“å®¶æƒé‡ (å¦‚æœä½¿ç”¨)                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              æŸå¤±è®¡ç®—                            â”‚
    â”‚                                                 â”‚
    â”‚  ä¸»æŸå¤±: CrossEntropyLoss(logits, targets)       â”‚
    â”‚  è¾…åŠ©æŸå¤± (MoE): aux_loss = Î£ expert_usageÂ²     â”‚
    â”‚  æ€»æŸå¤±: total_loss = main_loss + aux_loss      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              åå‘ä¼ æ’­                            â”‚
    â”‚                                                 â”‚
    â”‚  loss.backward() â†’ è®¡ç®—æ¢¯åº¦                     â”‚
    â”‚  â€¢ æ¢¯åº¦è£å‰ª (Gradient Clipping)                â”‚
    â”‚  â€¢ æ¢¯åº¦ç´¯ç§¯ (Gradient Accumulation)            â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              å‚æ•°æ›´æ–°                            â”‚
    â”‚                                                 â”‚
    â”‚  optimizer.step() â†’ æ›´æ–°å‚æ•°                    â”‚
    â”‚  â€¢ AdamW ä¼˜åŒ–å™¨                                â”‚
    â”‚  â€¢ å­¦ä¹ ç‡è°ƒåº¦ (LR Scheduling)                  â”‚
    â”‚  â€¢ æƒé‡è¡°å‡ (Weight Decay)                     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
        ä¸‹ä¸€ä¸ªæ‰¹æ¬¡
```

### å†…å­˜ä¼˜åŒ–ç­–ç•¥
```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              å†…å­˜ç®¡ç†                            â”‚
    â”‚                                                 â”‚
    â”‚  â€¢ æ¢¯åº¦æ£€æŸ¥ç‚¹ (Gradient Checkpointing)          â”‚
    â”‚  â€¢ æ··åˆç²¾åº¦è®­ç»ƒ (FP16/BF16)                     â”‚
    â”‚  â€¢ é›¶å†—ä½™ä¼˜åŒ–å™¨ (ZeRO)                         â”‚
    â”‚  â€¢ æ¨¡å‹å¹¶è¡Œ (Model Parallelism)                â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 4. æ¶æ„ä¼˜åŠ¿åˆ†æ

### 4.1 æ€§èƒ½ä¼˜åŠ¿
| ç»„ä»¶ | ä¼˜åŠ¿ | å…·ä½“è¡¨ç° |
|------|------|----------|
| **RMSNorm** | è®¡ç®—æ•ˆç‡é«˜ | ç›¸æ¯” LayerNorm å‡å°‘çº¦ 7-64% è®¡ç®—é‡ |
| **RoPE** | ä½ç½®ç¼–ç ä¼˜è¶Š | æ”¯æŒé•¿åºåˆ—å¤–æ¨ï¼Œç›¸å¯¹ä½ç½®ä¿¡æ¯æ›´å¥½ |
| **SwiGLU** | æ¿€æ´»å‡½æ•°ä¼˜åŒ– | ç›¸æ¯” ReLU æå‡çº¦ 1-2% æ€§èƒ½ |
| **Flash Attention** | å†…å­˜æ•ˆç‡ | å‡å°‘ 5-20x å†…å­˜ä½¿ç”¨ï¼Œæé€Ÿ 2-4x |
| **GQA/MQA** | æ¨ç†åŠ é€Ÿ | å‡å°‘ KV ç¼“å­˜ï¼Œæå‡æ¨ç†é€Ÿåº¦ |
| **MoE** | å‚æ•°æ•ˆç‡ | åœ¨ç›¸åŒè®¡ç®—é‡ä¸‹è·å¾—æ›´å¤§æ¨¡å‹å®¹é‡ |

### 4.2 æ¶æ„ä¼˜åŠ¿
- **æ¨¡å—åŒ–è®¾è®¡**: æ˜“äºæ‰©å±•å’Œä¿®æ”¹
- **ç°ä»£åŒ–ç»„ä»¶**: é‡‡ç”¨æœ€æ–°çš„ä¼˜åŒ–æŠ€æœ¯
- **çµæ´»é…ç½®**: æ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼
- **é«˜æ•ˆæ¨ç†**: æ”¯æŒ KV ç¼“å­˜å’Œæµå¼ç”Ÿæˆ

## 5. æ¶æ„åŠ£åŠ¿åˆ†æ

### 5.1 å¤æ‚æ€§åŠ£åŠ¿
| æ–¹é¢ | åŠ£åŠ¿ | å½±å“ |
|------|------|------|
| **å®ç°å¤æ‚åº¦** | MoE é—¨æ§æœºåˆ¶å¤æ‚ | è°ƒè¯•å›°éš¾ï¼Œå®¹æ˜“å‡ºé”™ |
| **å†…å­˜ç®¡ç†** | å¤šç§ä¼˜åŒ–æŠ€æœ¯æ··åˆ | å†…å­˜ä½¿ç”¨æ¨¡å¼å¤æ‚ |
| **è¶…å‚æ•°è°ƒä¼˜** | å‚æ•°ä¼—å¤š | éœ€è¦å¤§é‡å®éªŒæ‰¾åˆ°æœ€ä¼˜é…ç½® |

### 5.2 è®­ç»ƒæŒ‘æˆ˜
- **MoE è´Ÿè½½å‡è¡¡**: ä¸“å®¶åˆ©ç”¨ä¸å‡è¡¡é—®é¢˜
- **æ¢¯åº¦åŒæ­¥**: åˆ†å¸ƒå¼è®­ç»ƒæ—¶æ¢¯åº¦åŒæ­¥å¤æ‚
- **æ”¶æ•›ç¨³å®šæ€§**: å¤šä¸ªæŸå¤±é¡¹éœ€è¦ä»”ç»†å¹³è¡¡

### 5.3 ç¡¬ä»¶è¦æ±‚
- **å†…å­˜éœ€æ±‚å¤§**: å³ä½¿æœ‰ä¼˜åŒ–ï¼Œå¤§æ¨¡å‹ä»éœ€å¤§é‡æ˜¾å­˜
- **é€šä¿¡å¼€é”€**: åˆ†å¸ƒå¼è®­ç»ƒæ—¶é€šä¿¡æˆæœ¬é«˜
- **ä¸“ç”¨ç¡¬ä»¶**: æŸäº›ä¼˜åŒ–éœ€è¦ç‰¹å®šç¡¬ä»¶æ”¯æŒ

## 6. æŠ€æœ¯å­¦ä¹ è·¯å¾„

### 6.1 åŸºç¡€çŸ¥è¯† (2-4å‘¨)
```
    æ•°å­¦åŸºç¡€
    â”œâ”€â”€ çº¿æ€§ä»£æ•°
    â”‚   â”œâ”€â”€ çŸ©é˜µè¿ç®—
    â”‚   â”œâ”€â”€ ç‰¹å¾å€¼åˆ†è§£
    â”‚   â””â”€â”€ å¥‡å¼‚å€¼åˆ†è§£
    â”œâ”€â”€ æ¦‚ç‡ç»Ÿè®¡
    â”‚   â”œâ”€â”€ æ¦‚ç‡åˆ†å¸ƒ
    â”‚   â”œâ”€â”€ è´å¶æ–¯å®šç†
    â”‚   â””â”€â”€ ä¿¡æ¯è®ºåŸºç¡€
    â””â”€â”€ å¾®ç§¯åˆ†
        â”œâ”€â”€ æ¢¯åº¦ä¸å¯¼æ•°
        â”œâ”€â”€ é“¾å¼æ³•åˆ™
        â””â”€â”€ ä¼˜åŒ–ç†è®º
```

**æ¨èèµ„æº**:
- ã€Šçº¿æ€§ä»£æ•°åŠå…¶åº”ç”¨ã€‹- Gilbert Strang
- ã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹- æèˆª
- Khan Academy çº¿æ€§ä»£æ•°è¯¾ç¨‹

### 6.2 æ·±åº¦å­¦ä¹ åŸºç¡€ (4-6å‘¨)
```
    æ·±åº¦å­¦ä¹ æ¡†æ¶
    â”œâ”€â”€ PyTorch åŸºç¡€
    â”‚   â”œâ”€â”€ å¼ é‡æ“ä½œ
    â”‚   â”œâ”€â”€ è‡ªåŠ¨å¾®åˆ†
    â”‚   â””â”€â”€ æ¨¡å‹æ„å»º
    â”œâ”€â”€ ç¥ç»ç½‘ç»œåŸç†
    â”‚   â”œâ”€â”€ å‰å‘ä¼ æ’­
    â”‚   â”œâ”€â”€ åå‘ä¼ æ’­
    â”‚   â””â”€â”€ ä¼˜åŒ–ç®—æ³•
    â””â”€â”€ å®è·µé¡¹ç›®
        â”œâ”€â”€ å›¾åƒåˆ†ç±»
        â”œâ”€â”€ æ–‡æœ¬åˆ†ç±»
        â””â”€â”€ åºåˆ—é¢„æµ‹
```

**æ¨èèµ„æº**:
- ã€Šæ·±åº¦å­¦ä¹ ã€‹- Ian Goodfellow
- PyTorch å®˜æ–¹æ•™ç¨‹
- CS231n æ–¯å¦ç¦è¯¾ç¨‹

### 6.3 Transformer ä¸“é¢˜ (6-8å‘¨)
```
    Transformer æ¶æ„
    â”œâ”€â”€ æ³¨æ„åŠ›æœºåˆ¶
    â”‚   â”œâ”€â”€ Self-Attention
    â”‚   â”œâ”€â”€ Multi-Head Attention
    â”‚   â””â”€â”€ Cross-Attention
    â”œâ”€â”€ ä½ç½®ç¼–ç 
    â”‚   â”œâ”€â”€ æ­£å¼¦ä½ç½®ç¼–ç 
    â”‚   â”œâ”€â”€ å­¦ä¹ ä½ç½®ç¼–ç 
    â”‚   â””â”€â”€ æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)
    â”œâ”€â”€ å½’ä¸€åŒ–æŠ€æœ¯
    â”‚   â”œâ”€â”€ LayerNorm
    â”‚   â”œâ”€â”€ RMSNorm
    â”‚   â””â”€â”€ Pre/Post-Norm
    â””â”€â”€ ä¼˜åŒ–æŠ€æœ¯
        â”œâ”€â”€ Flash Attention
        â”œâ”€â”€ æ¢¯åº¦æ£€æŸ¥ç‚¹
        â””â”€â”€ æ··åˆç²¾åº¦è®­ç»ƒ
```

**æ¨èèµ„æº**:
- "Attention Is All You Need" è®ºæ–‡
- "The Illustrated Transformer" åšå®¢
- Hugging Face Transformers åº“

### 6.4 ç°ä»£ LLM æŠ€æœ¯ (8-12å‘¨)
```
    å¤§è¯­è¨€æ¨¡å‹
    â”œâ”€â”€ æ¨¡å‹æ¶æ„æ¼”è¿›
    â”‚   â”œâ”€â”€ GPT ç³»åˆ—
    â”‚   â”œâ”€â”€ LLaMA ç³»åˆ—
    â”‚   â””â”€â”€ å…¶ä»–å¼€æºæ¨¡å‹
    â”œâ”€â”€ è®­ç»ƒæŠ€æœ¯
    â”‚   â”œâ”€â”€ é¢„è®­ç»ƒç­–ç•¥
    â”‚   â”œâ”€â”€ æŒ‡ä»¤å¾®è°ƒ (SFT)
    â”‚   â”œâ”€â”€ RLHF
    â”‚   â””â”€â”€ DPO
    â”œâ”€â”€ æ¨ç†ä¼˜åŒ–
    â”‚   â”œâ”€â”€ KV ç¼“å­˜
    â”‚   â”œâ”€â”€ æ¨¡å‹é‡åŒ–
    â”‚   â”œâ”€â”€ æ¨¡å‹å‰ªæ
    â”‚   â””â”€â”€ åŠ¨æ€æ‰¹å¤„ç†
    â””â”€â”€ é«˜çº§æŠ€æœ¯
        â”œâ”€â”€ æ··åˆä¸“å®¶ (MoE)
        â”œâ”€â”€ æ£€ç´¢å¢å¼º (RAG)
        â”œâ”€â”€ æ€ç»´é“¾ (CoT)
        â””â”€â”€ æ™ºèƒ½ä½“ (Agent)
```

**æ¨èèµ„æº**:
- LLaMA è®ºæ–‡ç³»åˆ—
- OpenAI GPT è®ºæ–‡ç³»åˆ—
- ã€Šå¤§è¯­è¨€æ¨¡å‹ç†è®ºä¸å®è·µã€‹

### 6.5 å®è·µé¡¹ç›®è·¯å¾„

#### åˆçº§é¡¹ç›® (å®ç°åŸºç¡€ç»„ä»¶)
1. **ä»é›¶å®ç° Attention**
   ```python
   # æ‰‹å†™å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
   class MultiHeadAttention(nn.Module):
       def __init__(self, d_model, n_heads):
           # å®ç°ç»†èŠ‚...
   ```

2. **å®ç° RMSNorm**
   ```python
   # å¯¹æ¯” LayerNorm å’Œ RMSNorm çš„æ€§èƒ½å·®å¼‚
   ```

3. **RoPE ä½ç½®ç¼–ç **
   ```python
   # ç†è§£å¤æ•°è¿ç®—åœ¨ä½ç½®ç¼–ç ä¸­çš„åº”ç”¨
   ```

#### ä¸­çº§é¡¹ç›® (æ„å»ºå®Œæ•´æ¨¡å‹)
1. **Mini Transformer**
   - ä»é›¶æ„å»ºä¸€ä¸ªå°å‹ Transformer
   - åœ¨ç®€å•ä»»åŠ¡ä¸Šè®­ç»ƒå’Œæµ‹è¯•

2. **æ–‡æœ¬ç”Ÿæˆå™¨**
   - å®ç°è‡ªå›å½’ç”Ÿæˆ
   - æ·»åŠ é‡‡æ ·ç­–ç•¥ (temperature, top-p)

3. **æ¨¡å‹ä¼˜åŒ–**
   - å®ç° Flash Attention
   - æ·»åŠ  KV ç¼“å­˜æœºåˆ¶

#### é«˜çº§é¡¹ç›® (å‰æ²¿æŠ€æœ¯)
1. **MoE å®ç°**
   - å®ç°ä¸“å®¶è·¯ç”±æœºåˆ¶
   - è´Ÿè½½å‡è¡¡ç­–ç•¥

2. **åˆ†å¸ƒå¼è®­ç»ƒ**
   - æ•°æ®å¹¶è¡Œ
   - æ¨¡å‹å¹¶è¡Œ
   - æµæ°´çº¿å¹¶è¡Œ

3. **æ¨ç†ä¼˜åŒ–**
   - æ¨¡å‹é‡åŒ–
   - æ¨ç†åŠ é€Ÿ
   - å†…å­˜ä¼˜åŒ–

### 6.6 å­¦ä¹ èµ„æºæ¨è

#### ğŸ“š å¿…è¯»è®ºæ–‡
1. **åŸºç¡€è®ºæ–‡**
   - "Attention Is All You Need" (Transformer)
   - "Language Models are Unsupervised Multitask Learners" (GPT-2)
   - "Training language models to follow instructions with human feedback" (InstructGPT)

2. **ä¼˜åŒ–æŠ€æœ¯**
   - "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
   - "RoFormer: Enhanced Transformer with Rotary Position Embedding"
   - "Root Mean Square Layer Normalization"

3. **æ··åˆä¸“å®¶**
   - "Switch Transformer: Scaling to Trillion Parameter Models"
   - "GLaM: Efficient Scaling of Language Models with Mixture-of-Experts"

#### ğŸ“ åœ¨çº¿è¯¾ç¨‹
- **æ–¯å¦ç¦ CS224N**: Natural Language Processing with Deep Learning
- **æ–¯å¦ç¦ CS229**: Machine Learning
- **DeepLearning.AI**: Deep Learning Specialization
- **Fast.ai**: Practical Deep Learning for Coders

#### ğŸ’» å®è·µå¹³å°
- **Hugging Face**: æ¨¡å‹åº“å’Œå·¥å…·
- **Google Colab**: å…è´¹ GPU ç¯å¢ƒ
- **Kaggle**: ç«èµ›å’Œæ•°æ®é›†
- **Papers With Code**: è®ºæ–‡å’Œä»£ç å®ç°

#### ğŸ“– æŠ€æœ¯åšå®¢
- **Jay Alammar**: The Illustrated Transformer ç³»åˆ—
- **Sebastian Ruder**: NLP è¿›å±•ç»¼è¿°
- **Andrej Karpathy**: AI æŠ€æœ¯è§£æ
- **Lilian Weng**: æ·±åº¦å­¦ä¹ æŠ€æœ¯æ€»ç»“

### 6.7 å­¦ä¹ å»ºè®®

#### å¾ªåºæ¸è¿›çš„å­¦ä¹ ç­–ç•¥
1. **ç†è®ºä¸å®è·µç»“åˆ**: æ¯å­¦ä¸€ä¸ªæ¦‚å¿µå°±åŠ¨æ‰‹å®ç°
2. **ä»ç®€å•åˆ°å¤æ‚**: å…ˆç†è§£åŸºç¡€å†å­¦ä¹ ä¼˜åŒ–æŠ€æœ¯
3. **å¤šçœ‹ä»£ç **: é˜…è¯»ä¼˜ç§€çš„å¼€æºå®ç°
4. **åŠ¨æ‰‹å®éªŒ**: åœ¨å°æ•°æ®é›†ä¸ŠéªŒè¯ç†è§£

#### å¸¸è§å­¦ä¹ è¯¯åŒº
- âŒ ç›´æ¥å­¦ä¹ æœ€æ–°æŠ€æœ¯è€Œå¿½ç•¥åŸºç¡€
- âŒ åªçœ‹è®ºæ–‡ä¸åŠ¨æ‰‹å®ç°  
- âŒ è¿‡åº¦å…³æ³¨ç»†èŠ‚è€Œå¿½ç•¥æ•´ä½“æ¶æ„
- âŒ ä¸æ³¨é‡æ•°å­¦åŸºç¡€çš„é‡è¦æ€§

#### å­¦ä¹ æ—¶é—´è§„åˆ’
- **æ¯æ—¥**: 1-2å°æ—¶ç†è®ºå­¦ä¹  + 1-2å°æ—¶ç¼–ç¨‹å®è·µ
- **æ¯å‘¨**: å®Œæˆä¸€ä¸ªå°é¡¹ç›®æˆ–è®ºæ–‡å¤ç°
- **æ¯æœˆ**: æ€»ç»“å­¦ä¹ æˆæœï¼Œè°ƒæ•´å­¦ä¹ è®¡åˆ’
- **æ¯å­£åº¦**: å®Œæˆä¸€ä¸ªç»¼åˆæ€§é¡¹ç›®

é€šè¿‡è¿™ä¸ªç³»ç»ŸåŒ–çš„å­¦ä¹ è·¯å¾„ï¼Œæ‚¨å¯ä»¥é€æ­¥æŒæ¡ MiniMind æ¶æ„èƒŒåçš„æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶å…·å¤‡ç‹¬ç«‹è®¾è®¡å’Œå®ç°å¤§è¯­è¨€æ¨¡å‹çš„èƒ½åŠ›ã€‚
