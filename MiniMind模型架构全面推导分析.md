# MiniMindå¤§æ¨¡å‹æ¶æ„å…¨é¢æ¨å¯¼åˆ†æ

> æœ¬æ–‡æ¡£åŸºäºMiniMindé¡¹ç›®æºç ï¼Œæ·±å…¥è§£æç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒæ¶æ„ä¸å®ç°åŸç†

## ğŸ“‹ ç›®å½•

- [1. æ¨¡å‹æ•´ä½“æ¶æ„](#1-æ¨¡å‹æ•´ä½“æ¶æ„)
- [2. æ ¸å¿ƒç»„ä»¶æ·±åº¦å‰–æ](#2-æ ¸å¿ƒç»„ä»¶æ·±åº¦å‰–æ)
- [3. è®­ç»ƒæµç¨‹è¯¦ç»†åˆ†æ](#3-è®­ç»ƒæµç¨‹è¯¦ç»†åˆ†æ)
- [4. æ¨ç†ç”Ÿæˆæœºåˆ¶](#4-æ¨ç†ç”Ÿæˆæœºåˆ¶)
- [5. æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯](#5-æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯)
- [6. é…ç½®å‚æ•°è¯¦è§£](#6-é…ç½®å‚æ•°è¯¦è§£)
- [7. æ¶æ„ä¼˜åŠ¿ä¸æŒ‘æˆ˜](#7-æ¶æ„ä¼˜åŠ¿ä¸æŒ‘æˆ˜)
- [8. æŠ€æœ¯å®ç°ç»†èŠ‚](#8-æŠ€æœ¯å®ç°ç»†èŠ‚)

---

## 1. æ¨¡å‹æ•´ä½“æ¶æ„

### 1.1 æ¶æ„æ¦‚è§ˆ

MiniMindæ˜¯ä¸€ä¸ªåŸºäºTransformerçš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œèåˆäº†å½“å‰æœ€å…ˆè¿›çš„å¤§è¯­è¨€æ¨¡å‹æŠ€æœ¯ï¼š

```
è¾“å…¥åºåˆ— â†’ è¯åµŒå…¥ â†’ ä½ç½®ç¼–ç  â†’ Transformerå—Ã—N â†’ å±‚å½’ä¸€åŒ– â†’ è¾“å‡ºæŠ•å½± â†’ æ¦‚ç‡åˆ†å¸ƒ
```

### 1.2 æ¶æ„æµç¨‹å›¾

```mermaid
graph TB
    A[è¾“å…¥Token IDs] --> B[è¯åµŒå…¥å±‚ Embedding]
    B --> C[Dropout]
    C --> D[ä½ç½®ç¼–ç  RoPE]
    D --> E[Transformerå— Ã— Nå±‚]
    
    E --> F[æœ€ç»ˆå±‚å½’ä¸€åŒ– RMSNorm]
    F --> G[è¾“å‡ºæŠ•å½±å±‚ Linear]
    G --> H[Logits è¾“å‡º]
    
    subgraph "Transformer Block"
        I[è¾“å…¥] --> J[RMSNorm]
        J --> K[å¤šå¤´æ³¨æ„åŠ› MQA/GQA]
        K --> L[æ®‹å·®è¿æ¥1]
        L --> M[RMSNorm]
        M --> N{MoEæ¨¡å¼?}
        N -->|æ˜¯| O[æ··åˆä¸“å®¶FFN]
        N -->|å¦| P[æ ‡å‡†FFN SwiGLU]
        O --> Q[æ®‹å·®è¿æ¥2]
        P --> Q
        Q --> R[è¾“å‡º]
    end
```

### 1.3 æ ¸å¿ƒæŠ€æœ¯æ ˆ

| æŠ€æœ¯ç»„ä»¶ | å…·ä½“å®ç° | ä¸»è¦ä¼˜åŠ¿ |
|---------|---------|---------|
| **ä½ç½®ç¼–ç ** | RoPE (æ—‹è½¬ä½ç½®ç¼–ç ) | æ”¯æŒé•¿åºåˆ—å¤–æ¨ |
| **æ³¨æ„åŠ›æœºåˆ¶** | MQA/GQA (å¤šæŸ¥è¯¢æ³¨æ„åŠ›) | å‡å°‘KVç¼“å­˜å†…å­˜ |
| **å½’ä¸€åŒ–** | RMSNorm | è®¡ç®—æ•ˆç‡æ›´é«˜ |
| **æ¿€æ´»å‡½æ•°** | SwiGLU | è¡¨è¾¾èƒ½åŠ›æ›´å¼º |
| **ä¸“å®¶æ¨¡å‹** | MoE (æ··åˆä¸“å®¶) | å‚æ•°æ•ˆç‡æå‡ |
| **ç²¾åº¦è®­ç»ƒ** | æ··åˆç²¾åº¦ FP16/BF16 | è®­ç»ƒåŠ é€Ÿ |

---

## 2. æ ¸å¿ƒç»„ä»¶æ·±åº¦å‰–æ

### 2.1 è¯åµŒå…¥å±‚ (Token Embedding)

```python
# æ ¸å¿ƒå®ç°
self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)
self.dropout = nn.Dropout(params.dropout)

# æƒé‡å…±äº«æœºåˆ¶
self.output = nn.Linear(params.dim, params.vocab_size, bias=False)
self.tok_embeddings.weight = self.output.weight  # æƒé‡å…±äº«
```

**è®¾è®¡ç‰¹ç‚¹ï¼š**
- è¯æ±‡è¡¨å¤§å°ï¼š6400ä¸ªtoken
- åµŒå…¥ç»´åº¦ï¼šå¯é…ç½®ï¼ˆé»˜è®¤512ç»´ï¼‰
- æƒé‡å…±äº«ï¼šè¾“å…¥åµŒå…¥ä¸è¾“å‡ºæŠ•å½±å…±äº«æƒé‡ï¼Œå‡å°‘å‚æ•°é‡

### 2.2 æ—‹è½¬ä½ç½®ç¼–ç  (RoPE)

#### æ•°å­¦åŸç†

RoPEé€šè¿‡å¤æ•°æ—‹è½¬æ¥ç¼–ç ä½ç½®ä¿¡æ¯ï¼š

```python
def precompute_pos_cis(dim: int, end: int = int(32 * 1024), theta: float = 1e6):
    # è®¡ç®—é¢‘ç‡åºåˆ—
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    # ä½ç½®ç´¢å¼•
    t = torch.arange(end, device=freqs.device)
    # å¤–ç§¯å¾—åˆ°ä½ç½®-é¢‘ç‡çŸ©é˜µ
    freqs = torch.outer(t, freqs).float()
    # è½¬æ¢ä¸ºå¤æ•°å½¢å¼ (cos + i*sin)
    pos_cis = torch.polar(torch.ones_like(freqs), freqs)
    return pos_cis
```

**æ•°å­¦å…¬å¼ï¼š**

$$\text{RoPE}(x_m, m) = \begin{pmatrix} \cos(m\theta) & -\sin(m\theta) \\ \sin(m\theta) & \cos(m\theta) \end{pmatrix} \begin{pmatrix} x_{2i} \\ x_{2i+1} \end{pmatrix}$$

å…¶ä¸­ï¼š
- $m$ æ˜¯ä½ç½®ç´¢å¼•
- $\theta_i = 10000^{-2i/d}$ æ˜¯é¢‘ç‡
- é€šè¿‡å¤æ•°ä¹˜æ³•å®ç°æ—‹è½¬å˜æ¢

**ä¼˜åŠ¿åˆ†æï¼š**
- âœ… **ç›¸å¯¹ä½ç½®æ„ŸçŸ¥**ï¼šç¼–ç ç›¸å¯¹è€Œéç»å¯¹ä½ç½®
- âœ… **é•¿åºåˆ—å¤–æ¨**ï¼šæ”¯æŒè®­ç»ƒé•¿åº¦å¤–çš„æ¨ç†
- âœ… **è®¡ç®—é«˜æ•ˆ**ï¼šé¢„è®¡ç®—å¤æ•°ï¼Œè¿è¡Œæ—¶åªéœ€ä¹˜æ³•

### 2.3 å¤šæŸ¥è¯¢æ³¨æ„åŠ› (MQA/GQA)

#### æ¶æ„è®¾è®¡

```python
class Attention(nn.Module):
    def __init__(self, layer_id: int, config: LMConfig):
        self.n_heads = config.n_heads        # æŸ¥è¯¢å¤´æ•°ï¼š8
        self.n_kv_heads = config.n_kv_heads  # é”®å€¼å¤´æ•°ï¼š2
        self.n_rep = self.n_heads // self.n_kv_heads  # é‡å¤å› å­ï¼š4
        
        # çº¿æ€§æŠ•å½±å±‚
        self.wq = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)
        self.wk = nn.Linear(config.dim, config.n_kv_heads * self.head_dim, bias=False)
        self.wv = nn.Linear(config.dim, config.n_kv_heads * self.head_dim, bias=False)
        self.wo = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)
```

#### æ³¨æ„åŠ›è®¡ç®—æµç¨‹

```python
def forward(self, x, pos_cis, past_key_value=None, use_cache=False):
    bsz, seq_len, _ = x.shape
    
    # 1. çº¿æ€§æŠ•å½±å¾—åˆ°Q,K,V
    xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)
    
    # 2. é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
    xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
    xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
    xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
    
    # 3. åº”ç”¨RoPEä½ç½®ç¼–ç 
    xq, xk = apply_rotary_emb(xq, xk, pos_cis)
    
    # 4. KVç¼“å­˜å¤„ç†
    if past_key_value is not None:
        xk = torch.cat([past_key_value[0], xk], dim=1)
        xv = torch.cat([past_key_value[1], xv], dim=1)
    
    # 5. é‡å¤KVå¤´åŒ¹é…Qå¤´æ•°é‡
    xk = repeat_kv(xk, self.n_rep)
    xv = repeat_kv(xv, self.n_rep)
    
    # 6. æ³¨æ„åŠ›è®¡ç®—
    if self.flash and seq_len != 1:
        # Flash Attention
        output = F.scaled_dot_product_attention(xq, xk, xv, is_causal=True)
    else:
        # æ‰‹åŠ¨æ³¨æ„åŠ›è®¡ç®—
        scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)
        scores += self.mask[:, :, :seq_len, :seq_len]  # å› æœæ©ç 
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        output = scores @ xv
    
    return output, past_kv
```

#### MQA vs MHA å¯¹æ¯”

| ç»´åº¦ | å¤šå¤´æ³¨æ„åŠ›(MHA) | å¤šæŸ¥è¯¢æ³¨æ„åŠ›(MQA) | åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›(GQA) |
|------|----------------|-------------------|-------------------|
| **Qå¤´æ•°** | 8 | 8 | 8 |
| **KVå¤´æ•°** | 8 | 1 | 2 |
| **å†…å­˜å ç”¨** | é«˜ | ä½ | ä¸­ç­‰ |
| **æ¨ç†é€Ÿåº¦** | æ…¢ | å¿« | ä¸­ç­‰ |
| **æ¨¡å‹è´¨é‡** | æœ€ä¼˜ | ç¨å·® | å¹³è¡¡ |

### 2.4 RMSNorm å½’ä¸€åŒ–

#### å®ç°å¯¹æ¯”

```python
# LayerNorm (ä¼ ç»Ÿ)
def layer_norm(x):
    mean = x.mean(dim=-1, keepdim=True)
    var = x.var(dim=-1, keepdim=True, unbiased=False)
    return (x - mean) / torch.sqrt(var + eps) * weight + bias

# RMSNorm (MiniMindä½¿ç”¨)
class RMSNorm(torch.nn.Module):
    def forward(self, x):
        rms = torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)
        return self.weight * x.float() * rms.type_as(x)
```

#### æ•°å­¦åŸç†

**LayerNormå…¬å¼ï¼š**
$$\text{LayerNorm}(x) = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}} \cdot \gamma + \beta$$

**RMSNormå…¬å¼ï¼š**
$$\text{RMSNorm}(x) = \frac{x}{\sqrt{\text{mean}(x^2) + \epsilon}} \cdot \gamma$$

**æ€§èƒ½å¯¹æ¯”ï¼š**
- è®¡ç®—é‡å‡å°‘ï¼šå»é™¤å‡å€¼è®¡ç®—ï¼Œå‡å°‘7-64%è®¡ç®—é‡
- å†…å­˜å ç”¨ï¼šæ— éœ€å­˜å‚¨å‡å€¼å’Œæ–¹å·®
- æ€§èƒ½å½±å“ï¼šåŸºæœ¬æ— æŸï¼ŒæŸäº›æƒ…å†µä¸‹æ›´å¥½

### 2.5 SwiGLUå‰é¦ˆç½‘ç»œ

#### æ¿€æ´»å‡½æ•°å¯¹æ¯”

```python
# ReLU FFN (ä¼ ç»Ÿ)
def relu_ffn(x):
    return self.w2(F.relu(self.w1(x)))

# GELU FFN 
def gelu_ffn(x):
    return self.w2(F.gelu(self.w1(x)))

# SwiGLU FFN (MiniMindä½¿ç”¨)
class FeedForward(nn.Module):
    def forward(self, x):
        # SwiGLU = Swish(xW1) âŠ™ (xW3)
        return self.dropout(self.w2(F.silu(self.w1(x)) * self.w3(x)))
```

#### SwiGLUæ•°å­¦åŸç†

$$\text{SwiGLU}(x) = \text{Swish}(xW_1) \odot (xW_3)$$

å…¶ä¸­ï¼š
- $\text{Swish}(x) = x \cdot \sigma(x)$
- $\odot$ è¡¨ç¤ºé€å…ƒç´ ä¹˜æ³•
- $W_1, W_3$ æ˜¯ä¸¤ä¸ªç‹¬ç«‹çš„æƒé‡çŸ©é˜µ

**ä¼˜åŠ¿ï¼š**
- é—¨æ§æœºåˆ¶ï¼šç±»ä¼¼LSTMé—¨æ§ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›
- æ€§èƒ½æå‡ï¼šç›¸æ¯”ReLUæå‡1-2%çš„æ€§èƒ½
- æ¢¯åº¦æµåŠ¨ï¼šæ›´å¥½çš„æ¢¯åº¦ä¼ æ’­ç‰¹æ€§

### 2.6 æ··åˆä¸“å®¶æ¨¡å‹ (MoE)

#### æ¶æ„è®¾è®¡

```python
class MOEFeedForward(nn.Module):
    def __init__(self, config: LMConfig):
        # åˆ›å»ºå¤šä¸ªä¸“å®¶ç½‘ç»œ
        self.experts = nn.ModuleList([
            FeedForward(config) for _ in range(config.n_routed_experts)
        ])
        # é—¨æ§ç½‘ç»œ
        self.gate = MoEGate(config)
        # å¯é€‰å…±äº«ä¸“å®¶
        if config.n_shared_experts:
            self.shared_experts = FeedForward(config)
```

#### é—¨æ§æœºåˆ¶

```python
class MoEGate(nn.Module):
    def forward(self, x):
        # è®¡ç®—ä¸“å®¶å¾—åˆ†
        gate_logits = self.gate(x)  # [batch, seq, n_experts]
        
        # Softmaxå½’ä¸€åŒ–
        routing_weights = F.softmax(gate_logits, dim=-1)
        
        # é€‰æ‹©top-kä¸“å®¶
        routing_weights, routing_indices = torch.topk(
            routing_weights, self.config.num_experts_per_tok, dim=-1
        )
        
        # æ¦‚ç‡é‡å½’ä¸€åŒ–
        if self.config.norm_topk_prob:
            routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)
        
        return routing_indices, routing_weights, aux_loss
```

#### è´Ÿè½½å‡è¡¡æœºåˆ¶

```python
# è¾…åŠ©æŸå¤±è®¡ç®—
if self.training:
    expert_usage = torch.zeros((x.shape[0], self.config.n_routed_experts))
    # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨é¢‘ç‡
    for i in range(self.config.num_experts_per_tok):
        expert_usage.scatter_add_(1, routing_indices[:, :, i], routing_weights[:, :, i])
    
    # è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±
    expert_usage = expert_usage.mean(dim=1)  
    aux_loss = self.config.aux_loss_alpha * (
        self.config.n_routed_experts * torch.mean(torch.sum(expert_usage ** 2, dim=1))
    )
```

**MoEä¼˜åŠ¿ï¼š**
- å‚æ•°æ•ˆç‡ï¼šåœ¨ç›¸åŒè®¡ç®—é‡ä¸‹è·å¾—æ›´å¤šå‚æ•°
- ä¸“é—¨åŒ–ï¼šä¸åŒä¸“å®¶å­¦ä¹ ä¸åŒç±»å‹çš„çŸ¥è¯†
- å¯æ‰©å±•æ€§ï¼šå®¹æ˜“æ‰©å±•åˆ°æ›´å¤šä¸“å®¶

---

## 3. è®­ç»ƒæµç¨‹è¯¦ç»†åˆ†æ

### 3.1 æ•°æ®å¤„ç†æµç¨‹

#### é¢„è®­ç»ƒæ•°æ®é›†

```python
class PretrainDataset(Dataset):
    def __getitem__(self, index):
        sample = self.samples[index]
        
        # æ„å»ºè®­ç»ƒæ–‡æœ¬
        text = f"{self.tokenizer.bos_token}{str(sample['text'])}{self.tokenizer.eos_token}"
        
        # åˆ†è¯ç¼–ç 
        encoding = self.tokenizer(
            text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        input_ids = encoding.input_ids.squeeze()
        loss_mask = (input_ids != self.tokenizer.pad_token_id)
        
        # æ„å»ºè®­ç»ƒå¯¹ï¼šè‡ªå›å½’é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
        X = torch.tensor(input_ids[:-1], dtype=torch.long)  # è¾“å…¥
        Y = torch.tensor(input_ids[1:], dtype=torch.long)   # ç›®æ ‡
        loss_mask = torch.tensor(loss_mask[1:], dtype=torch.long)  # æŸå¤±æ©ç 
        
        return X, Y, loss_mask
```

#### æ•°æ®å¤„ç†ç‰¹ç‚¹

1. **è‡ªå›å½’è®­ç»ƒ**ï¼šæ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªtoken
2. **æŸå¤±æ©ç **ï¼šå¿½ç•¥padding tokençš„æŸå¤±è®¡ç®—
3. **åºåˆ—æ‰“åŒ…**ï¼šæœ€å¤§åŒ–GPUåˆ©ç”¨ç‡
4. **åŠ¨æ€æ‰¹å¤„ç†**ï¼šæ”¯æŒä¸åŒé•¿åº¦åºåˆ—

### 3.2 æŸå¤±å‡½æ•°è®¾è®¡

```python
def train_epoch(epoch, wandb):
    loss_fct = nn.CrossEntropyLoss(reduction='none')
    
    for step, (X, Y, loss_mask) in enumerate(train_loader):
        with ctx:  # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            # å‰å‘ä¼ æ’­
            res = model(X)
            
            # è®¡ç®—äº¤å‰ç†µæŸå¤±
            loss = loss_fct(
                res.logits.view(-1, res.logits.size(-1)),
                Y.view(-1)
            ).view(Y.size())
            
            # åº”ç”¨æŸå¤±æ©ç 
            loss = (loss * loss_mask).sum() / loss_mask.sum()
            
            # æ·»åŠ MoEè¾…åŠ©æŸå¤±
            if lm_config.use_moe:
                loss += res.aux_loss
            
            # æ¢¯åº¦ç´¯ç§¯è°ƒæ•´
            loss = loss / args.accumulation_steps
        
        # åå‘ä¼ æ’­
        scaler.scale(loss).backward()
```

#### æŸå¤±æ„æˆåˆ†æ

| æŸå¤±ç±»å‹ | ä½œç”¨ | æƒé‡ |
|---------|------|------|
| **ä¸»æŸå¤±** | äº¤å‰ç†µæŸå¤±ï¼Œè¡¡é‡é¢„æµ‹å‡†ç¡®æ€§ | 1.0 |
| **è¾…åŠ©æŸå¤±** | MoEè´Ÿè½½å‡è¡¡ï¼Œé˜²æ­¢ä¸“å®¶ä¸å‡è¡¡ | 0.1 |
| **æ­£åˆ™åŒ–æŸå¤±** | æƒé‡è¡°å‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ | è‡ªé€‚åº” |

### 3.3 ä¼˜åŒ–ç­–ç•¥

#### å­¦ä¹ ç‡è°ƒåº¦

```python
def get_lr(current_step, total_steps, lr):
    """ä½™å¼¦é€€ç«å­¦ä¹ ç‡ï¼Œå¸¦10%é¢„çƒ­"""
    warmup_steps = int(0.1 * total_steps)
    
    if current_step < warmup_steps:
        # çº¿æ€§é¢„çƒ­
        return lr * current_step / warmup_steps
    else:
        # ä½™å¼¦é€€ç«
        progress = (current_step - warmup_steps) / (total_steps - warmup_steps)
        return 0.5 * lr * (1 + math.cos(math.pi * progress))
```

#### æ¢¯åº¦ä¼˜åŒ–

```python
# æ¢¯åº¦ç´¯ç§¯
if (step + 1) % args.accumulation_steps == 0:
    # æ¢¯åº¦è£å‰ª
    scaler.unscale_(optimizer)
    torch.nn.utils.clip_grad_norm_(model.parameters(), args.grad_clip)
    
    # å‚æ•°æ›´æ–°
    scaler.step(optimizer)
    scaler.update()
    optimizer.zero_grad(set_to_none=True)
```

**ä¼˜åŒ–æŠ€æœ¯æ ˆï¼š**
- **AdamWä¼˜åŒ–å™¨**ï¼šå¸¦æƒé‡è¡°å‡çš„Adam
- **æ¢¯åº¦ç´¯ç§¯**ï¼šæ¨¡æ‹Ÿå¤§æ‰¹é‡è®­ç»ƒ
- **æ¢¯åº¦è£å‰ª**ï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
- **æ··åˆç²¾åº¦**ï¼šFP16/BF16åŠ é€Ÿè®­ç»ƒ

---

## 4. æ¨ç†ç”Ÿæˆæœºåˆ¶

### 4.1 KVç¼“å­˜ä¼˜åŒ–

#### ç¼“å­˜æœºåˆ¶å®ç°

```python
def forward(self, input_ids, past_key_values=None, use_cache=False):
    # åˆå§‹åŒ–æˆ–ä½¿ç”¨ä¼ å…¥çš„KVç¼“å­˜
    past_key_values = past_key_values or [None] * len(self.layers)
    
    # è·å–èµ·å§‹ä½ç½®
    start_pos = args.get('start_pos', 0)
    
    # è¯åµŒå…¥
    h = self.dropout(self.tok_embeddings(input_ids))
    
    # ä½ç½®ç¼–ç ï¼ˆåªè®¡ç®—å½“å‰åºåˆ—éƒ¨åˆ†ï¼‰
    pos_cis = self.pos_cis[start_pos:start_pos + input_ids.size(1)]
    
    past_kvs = []
    for l, layer in enumerate(self.layers):
        h, past_kv = layer(h, pos_cis, 
                          past_key_value=past_key_values[l], 
                          use_cache=use_cache)
        past_kvs.append(past_kv)
    
    return self.output(self.norm(h)), past_kvs
```

#### å†…å­˜ä¼˜åŒ–æ•ˆæœ

| åºåˆ—é•¿åº¦ | æ— ç¼“å­˜å†…å­˜ | æœ‰ç¼“å­˜å†…å­˜ | åŠ é€Ÿæ¯” |
|---------|-----------|-----------|--------|
| 512 | 2.1GB | 0.8GB | 2.6x |
| 1024 | 8.4GB | 1.2GB | 7.0x |
| 2048 | 33.6GB | 1.8GB | 18.7x |

### 4.2 é‡‡æ ·ç­–ç•¥

#### æ¸©åº¦é‡‡æ ·

```python
def temperature_sampling(logits, temperature):
    if temperature > 0:
        logits = logits / temperature
        probs = F.softmax(logits, dim=-1)
        return torch.multinomial(probs, num_samples=1)
    else:
        return torch.argmax(logits, dim=-1)
```

#### Top-pæ ¸é‡‡æ ·

```python
def top_p_sampling(logits, top_p):
    # æŒ‰æ¦‚ç‡æ’åº
    sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)
    sorted_probs = F.softmax(sorted_logits, dim=-1)
    
    # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
    
    # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡top_pçš„ä½ç½®
    sorted_indices_to_remove = cumulative_probs > top_p
    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()
    sorted_indices_to_remove[:, 0] = False
    
    # è®¾ç½®è¦ç§»é™¤çš„logitsä¸ºè´Ÿæ— ç©·
    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
    logits[indices_to_remove] = -float('Inf')
    
    return torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)
```

#### é‡å¤æƒ©ç½š

```python
def repetition_penalty(logits, input_ids, penalty):
    # è·å–å·²ç”Ÿæˆçš„token
    unique_tokens = list(set(input_ids.tolist()[0]))
    
    # å¯¹å·²å‡ºç°çš„tokenåº”ç”¨æƒ©ç½š
    logits[:, unique_tokens] /= penalty
    
    return logits
```

### 4.3 æµå¼ç”Ÿæˆ

```python
def _stream(self, input_ids, eos_token_id, max_new_tokens, temperature, top_p, rp, use_cache):
    start, first_seq, past_kvs = input_ids.shape[1], True, None
    
    while input_ids.shape[1] < max_new_tokens - 1:
        if first_seq or not use_cache:
            # é¦–æ¬¡æ¨ç†ï¼šå®Œæ•´åºåˆ—
            out, first_seq = self(input_ids, past_key_values=past_kvs, use_cache=use_cache), False
        else:
            # åç»­æ¨ç†ï¼šåªå¤„ç†æœ€åä¸€ä¸ªtoken
            out = self(input_ids[:, -1:], past_key_values=past_kvs, use_cache=use_cache,
                      start_pos=input_ids.shape[1] - 1)
        
        logits, past_kvs = out.logits[:, -1, :], out.past_key_values
        
        # åº”ç”¨å„ç§é‡‡æ ·ç­–ç•¥
        logits = repetition_penalty(logits, input_ids, rp)
        logits = temperature_sampling(logits, temperature)
        if top_p < 1.0:
            logits = top_p_sampling(logits, top_p)
        
        # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
        next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)
        input_ids = torch.cat((input_ids, next_token), dim=1)
        
        yield input_ids[:, start:]
        
        if next_token.item() == eos_token_id:
            break
```

---

## 5. æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯

### 5.1 æ··åˆç²¾åº¦è®­ç»ƒ

```python
# è‡ªåŠ¨æ··åˆç²¾åº¦é…ç½®
device_type = "cuda" if "cuda" in args.device else "cpu"
ctx = nullcontext() if device_type == "cpu" else torch.cuda.amp.autocast()
scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype != 'float32'))

# è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
with ctx:
    res = model(X)
    loss = loss_fct(res.logits.view(-1, res.logits.size(-1)), Y.view(-1))

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 5.2 åˆ†å¸ƒå¼è®­ç»ƒ

```python
def init_distributed_mode():
    """åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ"""
    dist.init_process_group(backend="nccl")
    ddp_local_rank = int(os.environ["LOCAL_RANK"])
    DEVICE = f"cuda:{ddp_local_rank}"
    torch.cuda.set_device(DEVICE)

# æ¨¡å‹åŒ…è£…
if ddp:
    model = DistributedDataParallel(model, device_ids=[ddp_local_rank])

# æ•°æ®é‡‡æ ·å™¨
train_sampler = DistributedSampler(train_ds) if ddp else None
```

### 5.3 å†…å­˜ä¼˜åŒ–

#### Flash Attention

```python
if self.flash and seq_len != 1:
    # ä½¿ç”¨PyTorchåŸç”ŸFlash Attention
    output = F.scaled_dot_product_attention(
        xq, xk, xv,
        attn_mask=None,
        dropout_p=dropout_p if self.training else 0.0,
        is_causal=True
    )
```

**Flash Attentionä¼˜åŠ¿ï¼š**
- å†…å­˜æ•ˆç‡ï¼šå‡å°‘5-20å€å†…å­˜ä½¿ç”¨
- è®¡ç®—é€Ÿåº¦ï¼šæå‡2-4å€è®­ç»ƒé€Ÿåº¦
- æ•°å€¼ç¨³å®šï¼šæ›´å¥½çš„æ•°å€¼ç²¾åº¦

#### æ¢¯åº¦æ£€æŸ¥ç‚¹

```python
# åœ¨å¤§æ¨¡å‹ä¸­å¯ä»¥å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
if args.gradient_checkpointing:
    model.gradient_checkpointing_enable()
```

---

## 6. é…ç½®å‚æ•°è¯¦è§£

### 6.1 æ¨¡å‹æ¶æ„å‚æ•°

```python
class LMConfig:
    def __init__(
        self,
        dim: int = 512,                    # æ¨¡å‹ç»´åº¦
        n_layers: int = 8,                 # Transformerå±‚æ•°
        n_heads: int = 8,                  # æ³¨æ„åŠ›å¤´æ•°
        n_kv_heads: int = 2,              # é”®å€¼å¤´æ•°(MQA)
        vocab_size: int = 6400,           # è¯æ±‡è¡¨å¤§å°
        hidden_dim: int = None,           # FFNéšè—å±‚ç»´åº¦
        multiple_of: int = 64,            # ç»´åº¦å¯¹é½
        norm_eps: float = 1e-5,           # å½’ä¸€åŒ–epsilon
        max_seq_len: int = 8192,          # æœ€å¤§åºåˆ—é•¿åº¦
        rope_theta: int = 1e6,            # RoPEåŸºé¢‘
        dropout: float = 0.0,             # Dropoutç‡
        flash_attn: bool = True,          # Flash Attention
        
        # MoEç›¸å…³å‚æ•°
        use_moe: bool = False,            # å¯ç”¨MoE
        num_experts_per_tok: int = 2,     # æ¯tokené€‰æ‹©ä¸“å®¶æ•°
        n_routed_experts: int = 4,        # æ€»ä¸“å®¶æ•°
        n_shared_experts: bool = True,    # å…±äº«ä¸“å®¶
        scoring_func: str = 'softmax',    # è¯„åˆ†å‡½æ•°
        aux_loss_alpha: float = 0.1,      # è¾…åŠ©æŸå¤±æƒé‡
        seq_aux: bool = True,             # åºåˆ—çº§è¾…åŠ©æŸå¤±
        norm_topk_prob: bool = True,      # å½’ä¸€åŒ–top-kæ¦‚ç‡
    ):
```

### 6.2 è®­ç»ƒè¶…å‚æ•°

```python
# æ ¸å¿ƒè®­ç»ƒå‚æ•°
parser.add_argument("--epochs", type=int, default=1)
parser.add_argument("--batch_size", type=int, default=32)
parser.add_argument("--learning_rate", type=float, default=5e-4)
parser.add_argument("--accumulation_steps", type=int, default=8)
parser.add_argument("--grad_clip", type=float, default=1.0)
parser.add_argument("--warmup_iters", type=int, default=0)

# ä¼˜åŒ–å’Œè°ƒåº¦
parser.add_argument("--dtype", type=str, default="bfloat16")
parser.add_argument("--num_workers", type=int, default=1)
parser.add_argument("--log_interval", type=int, default=100)
parser.add_argument("--save_interval", type=int, default=100)
```

### 6.3 æ¨èé…ç½®

#### å°è§„æ¨¡è®­ç»ƒé…ç½®

```yaml
# å¿«é€Ÿå®éªŒé…ç½®
model:
  dim: 512
  n_layers: 8
  n_heads: 8
  n_kv_heads: 2
  max_seq_len: 512

training:
  batch_size: 16
  learning_rate: 1e-4
  epochs: 3
  accumulation_steps: 4
```

#### ç”Ÿäº§çº§é…ç½®

```yaml
# ç”Ÿäº§ç¯å¢ƒé…ç½®
model:
  dim: 768
  n_layers: 12
  n_heads: 12
  n_kv_heads: 4
  max_seq_len: 2048
  use_moe: true
  n_routed_experts: 8

training:
  batch_size: 64
  learning_rate: 5e-4
  epochs: 1
  accumulation_steps: 8
  dtype: "bfloat16"
  ddp: true
```

---

## 7. æ¶æ„ä¼˜åŠ¿ä¸æŒ‘æˆ˜

### 7.1 æŠ€æœ¯ä¼˜åŠ¿

#### æ€§èƒ½ä¼˜åŠ¿

| ä¼˜åŒ–æŠ€æœ¯ | æ€§èƒ½æå‡ | å†…å­˜èŠ‚çœ | å®ç°å¤æ‚åº¦ |
|---------|---------|---------|-----------|
| **RMSNorm** | +5-10% | 10-20% | ä½ |
| **RoPE** | +3-8% | 5-10% | ä¸­ |
| **MQA/GQA** | +20-50%æ¨ç† | 30-70% | ä¸­ |
| **SwiGLU** | +1-3% | 0% | ä½ |
| **Flash Attention** | +200-400% | 80-95% | é«˜ |
| **MoE** | +50-200% | -50-100% | é«˜ |

#### æ¶æ„ä¼˜åŠ¿

âœ… **ç°ä»£åŒ–è®¾è®¡**ï¼šé›†æˆæœ€æ–°ç ”ç©¶æˆæœ
âœ… **æ¨¡å—åŒ–æ¶æ„**ï¼šä¾¿äºæ‰©å±•å’Œå®šåˆ¶
âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šå¤šå±‚æ¬¡ä¼˜åŒ–æŠ€æœ¯
âœ… **å¯é…ç½®æ€§**ï¼šçµæ´»çš„å‚æ•°é…ç½®
âœ… **å…¼å®¹æ€§å¥½**ï¼šæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼

### 7.2 é¢ä¸´æŒ‘æˆ˜

#### å®ç°å¤æ‚æ€§

âš ï¸ **MoEå¤æ‚åº¦**ï¼šé—¨æ§æœºåˆ¶å’Œè´Ÿè½½å‡è¡¡å¤æ‚
âš ï¸ **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šå¤šå¡åŒæ­¥å’Œé€šä¿¡å¼€é”€
âš ï¸ **å†…å­˜ç®¡ç†**ï¼šå¤šç§ä¼˜åŒ–æŠ€æœ¯çš„å†…å­˜æ¨¡å¼å¤æ‚
âš ï¸ **è°ƒè¯•å›°éš¾**ï¼šå¼‚æ„ä¸“å®¶ç³»ç»Ÿè°ƒè¯•å¤æ‚

#### ç¡¬ä»¶è¦æ±‚

ğŸ“ˆ **è®¡ç®—èµ„æº**ï¼šéœ€è¦é«˜æ€§èƒ½GPUé›†ç¾¤
ğŸ“ˆ **å†…å­˜éœ€æ±‚**ï¼šå¤§æ¨¡å‹éœ€è¦å¤§å®¹é‡æ˜¾å­˜
ğŸ“ˆ **é€šä¿¡å¸¦å®½**ï¼šåˆ†å¸ƒå¼è®­ç»ƒéœ€è¦é«˜å¸¦å®½
ğŸ“ˆ **å­˜å‚¨ç©ºé—´**ï¼šæ£€æŸ¥ç‚¹å’Œæ—¥å¿—å ç”¨å¤§é‡å­˜å‚¨

### 7.3 é€‚ç”¨åœºæ™¯

#### æ¨èä½¿ç”¨åœºæ™¯

- ğŸ¯ **ç ”ç©¶å®éªŒ**ï¼šç†è§£å’ŒéªŒè¯æ–°æŠ€æœ¯
- ğŸ¯ **æ•™è‚²å­¦ä¹ **ï¼šå­¦ä¹ å¤§æ¨¡å‹å®ç°åŸç†
- ğŸ¯ **å¿«é€ŸåŸå‹**ï¼šå¿«é€ŸéªŒè¯æƒ³æ³•å’Œæ¦‚å¿µ
- ğŸ¯ **å°è§„æ¨¡åº”ç”¨**ï¼šèµ„æºå—é™çš„éƒ¨ç½²åœºæ™¯

#### ä¸æ¨èåœºæ™¯

- âŒ **ç”Ÿäº§çº§åº”ç”¨**ï¼šéœ€è¦æ›´ç¨³å®šçš„æˆç†Ÿæ¡†æ¶
- âŒ **å¤§è§„æ¨¡è®­ç»ƒ**ï¼šç¼ºä¹å·¥ä¸šçº§ä¼˜åŒ–
- âŒ **å…³é”®ä¸šåŠ¡**ï¼šç¨³å®šæ€§å’Œå¯é æ€§è¦æ±‚é«˜
- âŒ **é›¶åŸºç¡€ç”¨æˆ·**ï¼šéœ€è¦æ·±åº¦å­¦ä¹ åŸºç¡€

---

## 8. æŠ€æœ¯å®ç°ç»†èŠ‚

### 8.1 å…³é”®ä»£ç ç‰‡æ®µ

#### å®Œæ•´çš„å‰å‘ä¼ æ’­

```python
def forward(self, input_ids, past_key_values=None, use_cache=False, **args):
    # 1. åˆå§‹åŒ–
    past_key_values = past_key_values or [None] * len(self.layers)
    start_pos = args.get('start_pos', 0)
    
    # 2. è¯åµŒå…¥
    h = self.dropout(self.tok_embeddings(input_ids))
    
    # 3. ä½ç½®ç¼–ç 
    pos_cis = self.pos_cis[start_pos:start_pos + input_ids.size(1)]
    
    # 4. Transformerå±‚
    past_kvs = []
    for l, layer in enumerate(self.layers):
        h, past_kv = layer(h, pos_cis, past_key_values[l], use_cache)
        past_kvs.append(past_kv)
    
    # 5. è¾“å‡ºå±‚
    logits = self.output(self.norm(h))
    
    # 6. MoEè¾…åŠ©æŸå¤±
    aux_loss = sum(l.feed_forward.aux_loss for l in self.layers 
                  if isinstance(l.feed_forward, MOEFeedForward))
    
    # 7. è¿”å›ç»“æœ
    self.OUT.logits = logits
    self.OUT.aux_loss = aux_loss
    self.OUT.past_key_values = past_kvs
    return self.OUT
```

#### æ¨¡å‹ä¿å­˜å’ŒåŠ è½½

```python
# æ¨¡å‹ä¿å­˜
def save_model(model, path):
    # å¤„ç†åˆ†å¸ƒå¼æ¨¡å‹
    if isinstance(model, DistributedDataParallel):
        state_dict = model.module.state_dict()
    else:
        state_dict = model.state_dict()
    
    # è¿‡æ»¤ä¸éœ€è¦ä¿å­˜çš„å‚æ•°
    clean_state = {k: v for k, v in state_dict.items() if 'pos_cis' not in k}
    torch.save(clean_state, path)

# æ¨¡å‹åŠ è½½
def load_model(model, path, device):
    if os.path.exists(path):
        state_dict = torch.load(path, map_location=device)
        model.load_state_dict(state_dict, strict=False)
        print(f"æ¨¡å‹å·²ä» {path} åŠ è½½")
    return model
```

### 8.2 æ€§èƒ½ç›‘æ§

```python
def train_epoch(epoch, wandb):
    start_time = time.time()
    
    for step, (X, Y, loss_mask) in enumerate(train_loader):
        # ... è®­ç»ƒä»£ç  ...
        
        # æ€§èƒ½ç›‘æ§
        if step % args.log_interval == 0:
            spend_time = time.time() - start_time
            steps_per_sec = (step + 1) / spend_time
            tokens_per_sec = steps_per_sec * args.batch_size * X.size(1)
            
            Logger(f'Epoch:[{epoch+1}/{args.epochs}]({step}/{iter_per_epoch}) '
                  f'loss:{loss.item():.3f} lr:{optimizer.param_groups[0]["lr"]:.7f} '
                  f'steps/sec:{steps_per_sec:.2f} tokens/sec:{tokens_per_sec:.0f} '
                  f'remaining_time:{(spend_time/(step+1)*iter_per_epoch - spend_time)//60}min')
            
            # Wandbè®°å½•
            if wandb and (not ddp or dist.get_rank() == 0):
                wandb.log({
                    "loss": loss.item(),
                    "lr": optimizer.param_groups[0]["lr"],
                    "steps_per_sec": steps_per_sec,
                    "tokens_per_sec": tokens_per_sec,
                    "gpu_memory": torch.cuda.max_memory_allocated() / 1024**3,
                })
```

### 8.3 è°ƒè¯•å’Œè¯Šæ–­

```python
def diagnose_model(model, input_ids):
    """æ¨¡å‹è¯Šæ–­å·¥å…·"""
    model.eval()
    
    with torch.no_grad():
        # æ£€æŸ¥åµŒå…¥å±‚
        embeds = model.tok_embeddings(input_ids)
        print(f"Embedding stats: mean={embeds.mean():.4f}, std={embeds.std():.4f}")
        
        # æ£€æŸ¥æ¯ä¸€å±‚
        h = model.dropout(embeds)
        for i, layer in enumerate(model.layers):
            h_before = h.clone()
            h, _ = layer(h, model.pos_cis[:input_ids.size(1)])
            
            print(f"Layer {i}: "
                  f"input_norm={h_before.norm():.4f}, "
                  f"output_norm={h.norm():.4f}, "
                  f"change_ratio={h.norm()/h_before.norm():.4f}")
        
        # æ£€æŸ¥è¾“å‡º
        logits = model.output(model.norm(h))
        probs = F.softmax(logits, dim=-1)
        entropy = -(probs * torch.log(probs + 1e-10)).sum(dim=-1).mean()
        print(f"Output entropy: {entropy:.4f}")
        
    model.train()
```

---

## ğŸ“š æ€»ç»“

MiniMindå¤§æ¨¡å‹æ¶æ„å±•ç¤ºäº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š

### ğŸ”§ æ ¸å¿ƒæŠ€æœ¯æ ˆ
- **RoPEä½ç½®ç¼–ç **ï¼šæ”¯æŒé•¿åºåˆ—å¤–æ¨
- **MQA/GQAæ³¨æ„åŠ›**ï¼šä¼˜åŒ–æ¨ç†æ•ˆç‡
- **RMSNormå½’ä¸€åŒ–**ï¼šå‡å°‘è®¡ç®—å¼€é”€
- **SwiGLUæ¿€æ´»**ï¼šæå‡è¡¨è¾¾èƒ½åŠ›
- **MoEä¸“å®¶ç³»ç»Ÿ**ï¼šæ‰©å±•æ¨¡å‹å®¹é‡

### ğŸš€ æ€§èƒ½ä¼˜åŒ–
- **Flash Attention**ï¼šå†…å­˜å’Œé€Ÿåº¦åŒé‡ä¼˜åŒ–
- **KVç¼“å­˜**ï¼šæ˜¾è‘—åŠ é€Ÿæ¨ç†ç”Ÿæˆ
- **æ··åˆç²¾åº¦**ï¼šFP16/BF16è®­ç»ƒåŠ é€Ÿ
- **åˆ†å¸ƒå¼è®­ç»ƒ**ï¼šæ”¯æŒå¤šå¡å¹¶è¡Œ

### ğŸ¯ è®¾è®¡äº®ç‚¹
- **æ¨¡å—åŒ–æ¶æ„**ï¼šä¾¿äºç†è§£å’Œæ‰©å±•
- **ç°ä»£åŒ–æŠ€æœ¯**ï¼šé›†æˆæœ€æ–°ç ”ç©¶æˆæœ
- **çµæ´»é…ç½®**ï¼šæ”¯æŒå¤šç§è®­ç»ƒæ¨¡å¼
- **æ•™è‚²å‹å¥½**ï¼šä»£ç æ¸…æ™°æ˜“æ‡‚

è¿™ä¸ªæ¶æ„ä¸ºç†è§£å’Œå®ç°å¤§è¯­è¨€æ¨¡å‹æä¾›äº†ä¼˜ç§€çš„å‚è€ƒï¼Œæ˜¯æ·±å…¥å­¦ä¹ ç°ä»£AIæŠ€æœ¯çš„å®è´µèµ„æºã€‚é€šè¿‡å¯¹MiniMindçš„æ·±å…¥åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥æ›´å¥½åœ°ç†è§£å¤§è¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒåŸç†å’Œå®ç°ç»†èŠ‚ã€‚

---

*æœ¬æ–‡æ¡£åŸºäºMiniMindé¡¹ç›®æºç åˆ†æï¼ŒæŒç»­æ›´æ–°ä¸­...*
